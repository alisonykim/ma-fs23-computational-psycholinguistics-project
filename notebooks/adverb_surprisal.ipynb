{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating the surprisal of adverbs\n",
    "\n",
    "<strong>Author:</strong> Alison Y. Kim\n",
    "\n",
    "\n",
    "### Definition\n",
    "<strong>Surprisal</strong> (a.k.a. information content) is a measure of the amount of information gained when an event occurs which had some probability value associated with it. Mathematically, it can be represented as such: for some instance or outcome $ x_i $ of random variable $ X $, which can take on values $ x_1, x_2, ... $, and the probability of outcome $ x_{i} $, $ p(x_{i}) $, the surprisal of $ x_{i} $ is given by $$ h(x_{i}) = -\\log_{2}{p(x_{i})} \\text{ bits} $$\n",
    "* $ p(x_{i}) = 1 \\Rightarrow h(x_{i}) = 0 \\text{ bits} $\n",
    "* $ p(x_{i}) = 0 \\Rightarrow h(x_{i}) = \\infty \\text{ bits} $\n",
    "\n",
    "In our experiment, one of the values of interest is the surprisal of an adverb.\n",
    "\n",
    "### Methods\n",
    "\n",
    "To approximate token probabilities, we use <a href=\"https://huggingface.co/gpt2-large\">GPT-2 large</a>, an English language model (LM). It is accessible via the Hugging Face framework.<br>\n",
    "\n",
    "For tensor manipulation and operations, we use PyTorch.\n",
    "\n",
    "### Calculation: Sentential semantic complexity\n",
    "\n",
    "#### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alisonykim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "lm = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2-large')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define sentences\n",
    "\n",
    "Parallel corpora (```common``` = sentences with commonly appearing adverbs, ```rare``` = sentences with rarely appearing adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = [\n",
    "\t'They looked at each other happily.',\n",
    "\t'The experienced doctor performed the operation sucessfully.',\n",
    "\t'She did not move, continuing to stare at me passionately.',\n",
    "\t'The dogs barked unexpectedly.',\n",
    "\t'The man on the boat waved angrily.',\n",
    "\t'One important person has been continuously absent.',\n",
    "\t'By the swimming pool, the neighbour waited nervously.',\n",
    "\t'It is so easy to be occasionally charitable.',\n",
    "\t'She clings to her marriage desperately.',\n",
    "\t'In this country, racism is spreading constantly.',\n",
    "\t'The little girl screamed and stamped her foot emotionally.',\n",
    "\t'The guests avoided all political discussion carefully.',\n",
    "\t'The day started normally.',\n",
    "\t'She grabbed the microphone stepped onto the stage confidently.',\n",
    "\t'They have been misleading you most unacceptably.',\n",
    "\t'To meet the deadline, the team worked efficiently.',\n",
    "\t'The student studied for the exam carefully.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = [\n",
    "\t'They looked at each other amiably.',\n",
    "\t'The experienced doctor performed the operation dexterously.',\n",
    "\t'She did not move, continuing to stare at me belligerently.',\n",
    "\t'The dogs barked ferociously.',\n",
    "\t'The man on the boat waved affably.',\n",
    "\t'One important person has been conspicuously absent.',\n",
    "\t'By the swimming pool, the neighbour waited languidly.',\n",
    "\t'It is so easy to be vicariously charitable.',\n",
    "\t'She clings to her marriage tenaciously.',\n",
    "\t'In this country, racism has been spreading insidiously.',\n",
    "\t'The little girl screamed and stamped her foot petulantly.',\n",
    "\t'The guests avoided all political discussion sedulously.',\n",
    "\t'The day started mundanely.',\n",
    "\t'She grabbed the microphone and stepped onto the stage audaciously.',\n",
    "\t'They have been misleading you most egregiously.',\n",
    "\t'To meet the deadline, the team worked assiduously.',\n",
    "\t'The student studied for the exam sedulously.'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating adverb surprisal\n",
    "* <strong>Step 1:</strong> Tokenize sentences, i.e. map each token to an index in the LM's vocabulary.\n",
    "\t* <strong>Step 1.5:</strong> Extract the adverb from the sentence and tokenize it separately from the entire sentence. (Some words like \"sedulously\" are mapped to more than 1 item in GPT-2's vocabulary. Therefore, we must tokenize the adverb separately from the entire sentence. See: <strong>Note</strong>.)\n",
    "* <strong>Step 2:</strong> Add \\<BOS\\> and <\\EOS\\> tokens to the sequence so that the probabilities we get are properly conditioned on the preceding tokens.\n",
    "* <strong>Step 3:</strong> Calculate tokens' conditional logits (pre-softmax probabilities) under the LM.\n",
    "* <strong>Step 4:</strong> Shift the label (token IDs) and logits tensors.\n",
    "\t* <strong>Logic:</strong> Here, we are interested in the <em>conditional</em> probabilities of words. Therefore, the conditional probability of \\<BOS\\> given previous tokens does not make sense to consider (there are no tokens preceding \\<BOS\\> in the sentence). Similarly, the probabilities over time steps after \\<BOS\\> are not of interest. The labels and logits tensors are shifted accordingly.\n",
    "* <strong>Step 5:</strong> Calculate the negative log likelihood of the vocabulary item(s) corresponding to the adverb.\n",
    "* <strong>Step 6:</strong> Extract the logits corresponding to the token position(s) of the adverb.\n",
    "\n",
    "<strong>Note:</strong> Because some adverbs are mapped to more than 1 vocabulary item, the helper function below is designed to identify the vocabulary item(s) corresponding to the adverb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_adverb_ids(\n",
    "    sent: str,\n",
    "    tokenizer: PreTrainedTokenizerFast\n",
    ") -> Union[Tuple[str, List[int], List[int]], Tuple[str, None]]:\n",
    "    \"\"\"\n",
    "    Identify the vocabulary ID(s) corresponding to the adverb.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of adverb vocabulary ID(s) and adverb token position(s)\n",
    "    \"\"\"\n",
    "    sent_tokenized = nltk.word_tokenize(sent, language='english')\n",
    "    # Adverbs in this experiment end with 'ly'\n",
    "    adverb = next((token for token in sent_tokenized if token.endswith('ly')))\n",
    "    sent_ids = [tokenizer.bos_token_id] + tokenizer(sent)['input_ids'] + [tokenizer.eos_token_id]\n",
    "    # Reverse-engineer the encoding (via, surprise^, decoding) to find corresponding ID(s)\n",
    "    # ^No pun intended\n",
    "    for i in range(len(sent_ids)-1):\n",
    "        curr_id_decoded = tokenizer.decode(sent_ids[i]).strip()\n",
    "        next_id_decoded = tokenizer.decode(sent_ids[i+1]).strip()\n",
    "        if curr_id_decoded == adverb:\n",
    "            return adverb, [sent_ids[i]], [i]\n",
    "        elif curr_id_decoded + next_id_decoded == adverb:\n",
    "            return adverb, sent_ids[i:i+2], [i, i+1]\n",
    "        elif len(curr_id_decoded + next_id_decoded) < len(adverb) and curr_id_decoded + next_id_decoded in adverb: # Substring in ```adverb``` but not equal\n",
    "            try:\n",
    "                next_next_id_decoded = tokenizer.decode(sent_ids[i+2]).strip()\n",
    "                if curr_id_decoded + next_id_decoded + next_next_id_decoded == adverb:\n",
    "                    return adverb, sent_ids[i:i+3], [i, i+1, i+2]\n",
    "            except IndexError:\n",
    "                return adverb, sent_ids[i:i+2], [i, i+1]\n",
    "    return adverb, None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common adverb sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverb_surprisal_common = dict()\n",
    "for sent in common:\n",
    "\t# Step 1\n",
    "\tsent_tokenized = tokenizer(sent)['input_ids']\n",
    "\t# Step 1.5\n",
    "\tadverb, adverb_tokenized, adverb_time_step = identify_adverb_ids(sent, tokenizer)\n",
    "\t# Step 2\n",
    "\tinput_ids = torch.tensor([tokenizer.bos_token_id] + sent_tokenized + [tokenizer.eos_token_id])\n",
    "\t# Step 3\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = lm(input_ids, labels=input_ids)\n",
    "\t# Step 4\n",
    "\tlabels_shifted = input_ids[..., 1:].contiguous()\n",
    "\tadverb_time_step_shifted = [time_step + 1 for time_step in adverb_time_step] # Shift time steps\n",
    "\tlogits = outputs['logits']\n",
    "\tlogits_shifted = logits[..., :-1, :].contiguous()\n",
    "\tassert logits_shifted.size(0) == labels_shifted.size(0) # As many labels as logits\n",
    "\t# Step 5\n",
    "\tnlls = -1 * torch.log_softmax(logits, dim=-1) # Token surprisal\n",
    "\t# Step 6\n",
    "\tadverb_surprisal_common[adverb] = [\n",
    "\t\tnlls[time_step][adverb_id].item()\n",
    "\t\tfor time_step, adverb_id in zip(adverb_time_step_shifted, adverb_tokenized)\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happily': [16.239721298217773],\n",
       " 'sucessfully': [16.785587310791016, 20.889488220214844, 17.696590423583984],\n",
       " 'passionately': [17.545166015625],\n",
       " 'unexpectedly': [15.709169387817383],\n",
       " 'angrily': [15.555511474609375],\n",
       " 'absent': [12.684146881103516],\n",
       " 'nervously': [15.868431091308594],\n",
       " 'charitable': [11.97054672241211],\n",
       " 'desperately': [13.908486366271973],\n",
       " 'constantly': [15.275320053100586],\n",
       " 'emotionally': [14.575103759765625],\n",
       " 'carefully': [14.272598266601562],\n",
       " 'normally': [15.229681015014648],\n",
       " 'confidently': [16.01131820678711],\n",
       " 'unacceptably': [18.11367416381836, 20.517274856567383, 18.268041610717773],\n",
       " 'efficiently': [16.871641159057617]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adverb_surprisal_common"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rare adverb sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverb_surprisal_rare = dict()\n",
    "for sent in rare:\n",
    "\t# Step 1\n",
    "\tsent_tokenized = tokenizer(sent)['input_ids']\n",
    "\t# Step 1.5\n",
    "\tadverb, adverb_tokenized, adverb_time_step = identify_adverb_ids(sent, tokenizer)\n",
    "\t# Step 2\n",
    "\tinput_ids = torch.tensor([tokenizer.bos_token_id] + sent_tokenized + [tokenizer.eos_token_id])\n",
    "\t# Step 3\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = lm(input_ids, labels=input_ids)\n",
    "\t# Step 4\n",
    "\tlabels_shifted = input_ids[..., 1:].contiguous()\n",
    "\tadverb_time_step_shifted = [time_step + 1 for time_step in adverb_time_step] # Shift time steps\n",
    "\tlogits = outputs['logits']\n",
    "\tlogits_shifted = logits[..., :-1, :].contiguous()\n",
    "\tassert logits_shifted.size(0) == labels_shifted.size(0) # As many labels as logits\n",
    "\t# Step 5\n",
    "\tnlls = -1 * torch.log_softmax(logits, dim=-1) # Token surprisal\n",
    "\t# Step 6\n",
    "\tadverb_surprisal_rare[adverb] = [\n",
    "\t\tnlls[time_step][adverb_id].item()\n",
    "\t\tfor time_step, adverb_id in zip(adverb_time_step_shifted, adverb_tokenized)\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amiably': [12.872679710388184, 21.171295166015625],\n",
       " 'dexterously': [15.868247985839844, 17.799373626708984],\n",
       " 'belligerently': [18.694435119628906, 22.017610549926758, 14.72154426574707],\n",
       " 'ferociously': [24.02274513244629, 16.825103759765625, 18.945785522460938],\n",
       " 'affably': [14.149466514587402, 19.266386032104492],\n",
       " 'absent': [13.07290267944336],\n",
       " 'languidly': [15.334259986877441, 16.413345336914062, 14.369363784790039],\n",
       " 'charitable': [12.827916145324707],\n",
       " 'tenaciously': [10.272384643554688, 21.092113494873047],\n",
       " 'insidiously': [21.785551071166992, 17.391551971435547, 19.25149154663086],\n",
       " 'petulantly': [24.11342430114746, 17.563024520874023, 19.76900863647461],\n",
       " 'sedulously': [12.262739181518555, 19.376073837280273],\n",
       " 'mundanely': [17.870346069335938, 13.98317813873291],\n",
       " 'audaciously': [12.411210060119629, 21.852031707763672],\n",
       " 'egregiously': [15.909463882446289, 18.83255386352539],\n",
       " 'assiduously': [24.112695693969727, 17.81847381591797, 21.968400955200195]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adverb_surprisal_rare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
